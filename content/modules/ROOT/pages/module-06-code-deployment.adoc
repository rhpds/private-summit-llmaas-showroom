:imagesdir: ../assets/images
[#code-deployment]
= Building and Deploying an MCP Server

In the last module, you built and ran a fun application using our MaaS model endpoint to power an agentic AI code extension.

Now, we'll take the next step: building and deploying tooling that teaches our model how to understand interact with real-world systems.

Your task: deploy a **Model Context Protocol (MCP)** server that lets your model interact with your company's Slack workspace.

This is your chance to shape how AI applications are actually built, tested, and deployed — not just in theory, but as part of a larger, evolving platform.

== What's an MCP Server? (The Plain-English version)

Think of MCP servers as plugins for AI. They give you model safe, governed access to things like databases, internal applications, or company Slack instances.

* Without MCP: the model can only talk.
* With MCP: the model can act-fetch info or take approved actions-under platform controls.

== Why Deploy MCP on OpenShift AI?

* **Security and Governance**: Namespaces, RBAC, and network policies keep tools isolated and controlled.
* **Hybrid cloud**: Consisten deployment across on-prem, cloud or both.
* **Observability**: See exactly what ran, where and when.
* **Scalability**: Roll from a demo pod to a fleet when the use case proves out.

Takeaway: The right AI platform choice determines whether "AI with tools" is a one-off demo or a repeatable, secure enterprise capability. 

== Explore the Deployment with Continue

Open the `llama-stack-on-ocp` -> `slack-mcp` folder. 

image:code/slack-mcp-folder.png[width="50%"]

=== Explain Code

* Click open the `slack-deployment.yaml` file.

image:code/slack_deployment_file.png[width="50%"]

* Highlight the entire file and right-click. Select "Add Highlighted Code to Context".

image:code/right-click-popup.png[width="50%"]

You may explore the Continue shortcuts however you'd like as we did in the last exercise. The selected text will be sent to the chat interface of the Continue extension. 

* In the chat window, add the following:

[source,text,role="execute"]
----
Explain this code.
----

image:code/explain_code.png[width="50%"]

* Press `Enter` to send the message.

* You may repeat this action with each file in the slack-mcp folder. 

* Read the README.md file to understand the deployment a bit better and the tools available.

== Deploy the Slack MCP Server

* In the terminal view, paste the following command to deploy the MCP server:

[source,console,role="execute"]
----
oc apply -k /projects/llama-stack-on-ocp/slack-mcp/ -n lls-demo
----

This will create the mcp server deployment and service.

image:code/successful_deploy.png[width="75%"]

== Verify successful deployment

1. In the terminal, run:

[source,console,role="execute"]
----
oc get pods -n lls-demo
----

This will show all pods in the namespace within which we just deployed our slack mcp server. You should see our `slack-mcp-server` pod up and running, or in the process of starting:

image::code/mcp_status.png[width="75%"]

== Wrap Up: What You Did

In this module, you:

* Learned what an MCP server is and why it matters in enterprise AI.

* Deployed an MCP server on OpenShift AI to extend your model’s capabilities.

* Connected the model to a real-world system (Slack) for governed, auditable interaction.

**Why it matters:** You just demonstrated how an AI developer can safely integrate enterprise systems into AI workflows using OpenShift AI — turning a conversational model into an action-capable assistant.