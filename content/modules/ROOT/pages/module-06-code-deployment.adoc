:imagesdir: ../assets/images
[#code-deployment]
= Building and Deploying an MCP Server

In the last module, you built and ran a fun application using our MaaS model endpoint to power an agentic AI code extension.

Now, we'll take the next step: building and deploying tooling that teaches our model how to understand interact with real-world systems.

Your task: deploy a **Model Context Protocol (MCP)** server that lets your model interact with your company's Slack workspace.

This is your chance to shape how AI applications are actually built, tested, and deployed — not just in theory, but as part of a larger, evolving platform.

== What's an MCP Server? (The Plain-English version)

Think of MCP servers as plugins for AI. They give your model safe, governed access to things like databases, internal applications, or company Slack instances.

* **Without MCP**: the model can only talk.
* **With MCP**: the model can act-fetch info or take approved actions-under platform controls.

== Why Deploy MCP on OpenShift AI?

* **Security and Governance**: Namespaces, RBAC, and network policies keep tools isolated and controlled.
* **Hybrid cloud**: Consisten deployment across on-prem, cloud or both.
* **Observability**: See exactly what ran, where and when.
* **Scalability**: Roll from a demo pod to a fleet when the use case proves out.

Takeaway: The right AI platform choice determines whether "AI with tools" is a one-off demo or a repeatable, secure enterprise capability. 

== Explore the Deployment with Continue

Open the `llama-stack-on-ocp` -> `slack-mcp` folder. 

image:code/slack-mcp-folder.png[width="50%"]

=== Explain Code

* Click open the `slack-deployment.yaml` file.

image:code/slack_deployment_file.png[width="50%"]

* Highlight the entire file and right-click. Select "Add Highlighted Code to Context".

image:code/right-click-popup.png[width="50%"]

You may explore the Continue shortcuts however you'd like as we did in the last exercise. The selected text will be sent to the chat interface of the Continue extension. 

* In the chat window, add the following:

[source,text,role="execute"]
----
Explain this code in simple terms.
----

image:code/explain_code.png[width="50%"]

* Press `Enter` to send the message.

* You may repeat this action with each file in the slack-mcp folder or specific sections of the code. 

* Read the README.md file to understand the deployment a bit better and the tools available.

IMPORTANT: Do not edit any of the code. It is in working order. If you do make changes, let us know and we can help you revert them if you are unsure how.

== Deploy the Slack MCP Server

* In the terminal view, paste the following command to deploy the MCP server:

[source,console,role="execute"]
----
oc apply -k /projects/llama-stack-on-ocp/slack-mcp/ -n lls-demo
----

This will create the mcp server deployment and service.

image:code/successful_deploy.png[width="75%"]

== Verify successful deployment

1. In the terminal, run:

[source,console,role="execute"]
----
oc get pods -n lls-demo
----

This will show all pods in the namespace within which we just deployed our slack mcp server. You should see our `slack-mcp-server` pod up and running, or in the process of starting:

image::code/mcp_status.png[width="75%"]

== Wrap Up: What You Did

In this module, you:

* Learned what an MCP server is and why it matters in enterprise AI.

* Deployed an MCP server on OpenShift AI to extend your model’s capabilities.

* Connected the model to a real-world system (Slack) for governed, auditable interaction.

**Why it matters:** You just demonstrated how an AI developer can safely integrate enterprise systems into AI workflows using OpenShift AI — turning a conversational model into an action-capable assistant.