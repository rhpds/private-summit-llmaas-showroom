:imagesdir: ../assets/images
[#agentic-ai]
= System Administration with Agentic AI 

In the last module, you stepped into the developer role building applications using private LLM endpoints. Now, you'll become a **DevOps engineer** focused on observability, diagnostics, and operational scale. 

But you won't go at it alone, you'll work with agentic AI tooling to navigate your OpenShift environment in real time - just like a modern DevOps team building insight-driven automation.

== Why Agentic AI for Admins?

Think of agentic AI as your copilot for cluster operations - ready to answer complex infrastructure questions using real-time telemetry and reasoning. 

Instead of navigating complex dashboards, YAML, and shell scripts, you'll try something new:

* “What’s consuming the most memory in our dev cluster right now?”

* “Were there any pod crashes in the last 30 minutes?”

* “Why did that Job fail this morning?”

These natural-language prompts get translated into real queries and results. The outcome? Faster issue resolution, increased system transparency, and lower barrier to operational insights for both technical and non-technical users.

That's an example of the power of agentic AI.

== Meet Your New Toolkit

In this module, we'll use a preconfigured stack that includes:

* **https://github.com/meta-llama/llama-stack[Llama Stack]** - a flexible open-source framework developed by Meta to simplify the creation and deployment of advanced AI applications.
** LlamaStack provides a unified API layer for inference, RAG, agents, tools, safety, evals and telemetry.
** **Why It Matters for Enterprises**:
*** Codifies best practices across Gen AI tools
*** Enables reproducible, explainable, and extensible AI workflows
*** In short, it lets you focus on value creation, not toolchain assembly.

// IMPORTANT: You could also use frameworks like LangChain or CrewAI instead of LlamaStack with OpenShift AI. All of these tools help you build agentic AI workflows with reasoning, tool use, and orchestration. LlamaStack is Red Hat's recommended, and supported, framework.

* **OpenShift MCP Server**  - allows our LLM agent to interact with our live OpenShift cluster. This MCP server is namespace-scoped. We will use it to interact with the OpenShift resources within the `lls-demo` namespace.

* **Slack MCP Server** - allows our LLM agent to interact with a Slack workspace. This MCP server is namespace-scoped as well. We will use it to interact with a specific Slack workspace for which we have authorization configured.

* **https://huggingface.co/meta-llama/Llama-Guard-3-1B[Llama Guard]** - optional input/output filtering for responsible AI interactions using another safety-trained LLM that is being served on our cluster.

== Tying it Back to our MaaS Model Endpoint

You can connect your Llama Stack server deployment to any model endpoint you desire, like our MaaS model endpoint configured and utilized in the previous modules. For the purposes of this workshop, this is already configured for us. 

// == Connect Your Model to LlamaStack

// If you did not save your MaaS model endpoint URL, navigate back to the 3Scale developer portal to grab it:

// Developer Portal: https://maas.{openshift_cluster_ingress_domain}[https://maas.{openshift_cluster_ingress_domain},window=_blank].

// === Add Model Endpoint to LlamaStack Distribution File

// * Go back to the OpenShift console: 

// https://console-openshift-console.{openshift_cluster_ingress_domain}/[https://console-openshift-console.{openshift_cluster_ingress_domain}/,window=_blank].

// * In the Administrator perspective, select API Explorer.

// image:llama/api_explorer.png[width="50%"]

// * Search `llamastackdistribution` in the search bar and select the resource.

// image:llama/llamastackdistribution.png[width="50%"]

// * Ensure you are in the right project. Type `lls-demo` in the project search bar.

// image::llama/lls-project.png[width="50%"]

// * Select `Instances` and the available instance.

// image:llama/llamastackinstance.png[width="50%"]

// * Select `YAML` and scroll down to the highlighted section of text.

// image:llama/lsd_yaml.png[width="75%"]

// * In place of the existing Granite URL, input your endpoint URL from the 3scale developer portal. Ensure `/v1` is appended to the string.

// image:llama/maas_endpoint.png[width="75%"]

// * Click save

// image:llama/save_yaml.png[width="50%"]

// // TODO: Add this section to config file to remove the need to do this manually during workshop

// === Add Slack MCP Server to LlamaStack Configuration

// * Click on `Workloads` -> `ConfigMaps`

// image:llama/configmap-nav.png[width="50%"]

// * Find our `llama-stack-config`

// image::llama/lls_config.png[width="50%"]

// * Click on the `yaml` tab.

// image::llama/yaml-tab.png[width="50%"]

// * Add the following to the end of llama-stack-config `ConfigMap` in the `tool_groups` section:

// [source,console,role=execute,subs=attributes+]
// ----
// - toolgroup_id: mcp::slack
//     provider_id: model-context-protocol
//     mcp_endpoint:
//     uri: "http://slack-mcp-server:80/sse"
// ----

// image:llama/configmap_tool.png[width="50%"]

// Ensure the tab indentations match the example above.

// * Click `Save` to persist the changes.

== View Your Deployment

* Go back to the OpenShift console: 

https://console-openshift-console.{openshift_cluster_ingress_domain}/[https://console-openshift-console.{openshift_cluster_ingress_domain}/,window=_blank].

* Now, select the `Developer` perspective.

image:llama/dev_perspective.png[width="50%"]

* In case you are not in our specific project where the Llama Stack resources are deployed, search for the `lls-demo` namespace:

image:llama/find-namespace.png[width="50%"]

* Select the `Topology` tab in the navigation bar as seen above.

In the Topology view, you will see four pods:

* **Llama Stack**: core server that connects Gen AI models to real-world tools and services. Our Llama Stack server handles the complex orchestration of turning natural language requests into real API calls, tool calls, and responses while maintaining context and security.
* **OCP MCP Server**: an MCP Server with tools to help our model interact with and understand OpenShift.
* **Slack MCP Server**: an MCP Server with tools to help our model interact with and understand Slack.
* **LlamaStack Playground**: A streamlit UI to interact with the system.

image::llama/see_topology.png[width="75%"]

Feel free to poke around and explore the deployment.

* Select the LlamaStack playground hyperlink to open the UI.

image:llama/playground_link.png[width="50%"]

Now you will see the "playground" user interface. This application was created in the upstream project for the purposes of demonstration and experimentation and is **not** a supported component of our downstream OpenShift AI product.

image::llama/playground_ui.png[width="75%"]

== Configure the AI Agent

Within the application you'll find a familiar chat interface with some selection options on the left-hand side.

* Select our model from the drop down

[.bordershadow]
image::llama/model_selection.png[width="50%"]

* Set `Processing mode` -> `Agent-based`, giving us access to the tools we have configured via the MCP servers.

image::llama/agent_selection.png[width="50%"]

* Enable the OpenShift MCP tool group.

image::llama/mcp_servers.png[width="50%"]

* Once the MCP server is selected, you can peruse the active tools available.

image:llama/active_tools.png[width="50%"]

Everything else can remain unchanged.

== Investigate our OpenShift Resources

The active tools information will give you guidance into how to interact with the model in chat to activate the tool calls correctly.

NOTE: Our LlamaStack deployment is namespace-scoped. Therefore, in this activity, we will only be able to interact with the OpenShift resources within the `lls-demo` namespace containing the LlamaStack server and playground.

In the chat, enter:

[source,console,role=execute,subs=attributes+]
----
List all pods in the lls-demo namespace.
----

Response output will vary. But you will see it activate the tool, and give you a response. Something like this:

image::llama/ocp_response_example.png[width="75%"]

Let's try something else:

[source,console,role=execute,subs=attributes+]
----
Get logs for the <ocp-mcp-server-pod-name> pod in the lls-demo namespace.
----

IMPORTANT: You will need to replace the `<ocp-mcp-server-pod-name>` with the actual pod name. You can find the pod name from the response to the `list all pods` prompt.

You will again see that the associate tool is activated, and the model will then generate a response from the context provided by the tool call.

Feel free to experiment further with the tools available.

NOTE: We are using a small model, which is not optimal for agentic AI performance in production use cases. For demos and non-critical work, it can be quite impressive! However, some responses may be incomplete or inconsistent, and the model may hallucinate or misinterpret results if the tool output is vague or malformed or if we are asking it to engage with multiple MCP servers (like in this workshop!). The demonstration is meant to highlight the potential of natural language interfaces for interacting with infrastructure, and how emerging tools like LlamaStack and MCP can reduce the barrier to entry for understanding system behavior and save valuable time and effort.

=== Post a Message to our Slack Workspace

With our Slack MCP Server connected to Llama Stack, we can extend our agentic AI experience beyond Kubernetes and into team collaboration tools (among many other possibilities!).

This MCP server bridges your AI agent with a Slack workspace to fetch approved data.

**Why this matters:**

* SREs and DevOps teams often work across multiple collaboration channels.

* By giving your AI visibility into Slack, you can use natural language to check team communication spaces without switching tools.

==== Activate the Slack MCP Server

In the left-hand menu, select the `Slack MCP Server` tool group. This will clear the current chat. You may keep the OpenShift MCP Server enabled as well or deactivate it. 

image::llama/mcp_servers_2nd.png[width="50%"]

NOTE: If you experience hallucinations with both MCP servers enabled or after a few different chat interactions, you may need to refresh your browser to reset the chat.

==== In the LlamaStack Playground chat interface, type:

[source,console,role=execute,subs=attributes+]
----
List all Slack channels in our Slack workspace.
----

Now, let's post a message to our Slack workspace.

NOTE: Substitute `<insert-event-city-name>` with the city name of the event you are attending! Or write whatever appropriate message you fancy to our channel.

[source,console,role=execute,subs=attributes+]
----
Post a message to the #all-summit-connect-2025 channel: "Hi from <insert-event-city-name>".
----

=== Send logs to Slack

Now, let's try out a very real use case for this! It may not be done through a chat UI like this, but it's a good example of how you can use agentic AI to help you with your work. In a production environment, you would likely use a more robust automation to send logs or other information from your OpenShift cluster to Slack.

Remember your role! You are a DevOps engineer. Let's send the logs for the ocp-mcp-server pod to the #all-summit-connect-2025 channel.

If you do not have both MCP servers enabled, make sure they are now.

If you no longer have the pod logs, let's retrieve them again:

[source,console,role=execute,subs=attributes+]
----
Get logs for the <ocp-mcp-server-pod-name> pod in the lls-demo namespace.
----

NOTE: You will need to replace the `<ocp-mcp-server-pod-name>` with the actual pod name. You can find the pod name from the response to the list all pods prompt.

Now, in a second message, post the logs to the #all-summit-connect-2025 channel:

[source,console,role=execute,subs=attributes+]
----
Post this log information to the #all-summit-connect-2025 slack workspace channel.
----

Expected output should look something like this:

image::llama/post_message.png[width="75%"]

Don't forget, if you do not get this result it's okay! It's because of our small model. Try refreshing the window and trying again.

=== Add Responsible AI Shields

Enterprise AI deployments require robust safety measures, especially when AI agents have access to critical infrastructure. **Guardian models** like Llama Guard serve as intelligent safety filters that evaluate both user inputs and AI outputs in real-time.

To enforce guardrails on inputs and outputs, select the **Llama Guard** model under the `Input Shields` and `Output Shields` form fields:

image::llama/guards.png[width="50%"]

Test the guards by asking the AI to perform an inappropriate action - you'll see how Llama Guard intercepts and blocks problematic requests!

== Summary: What You Did

In this module, you:

* Acted as an SRE or DevOps practitioner using AI for cluster resource insight
* Integrated your own LLM with a tool-using agent.
* Explored OpenShift resources with natural language
* Interacted with a Slack workspace using natural language
* Added AI guardrails with input/output shields.

You just used AI to reduce operational complexity and speed up workflows! 